<!doctype html><html lang=en prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# profile: http://ogp.me/ns/profile#"><head><base href=https://www.aviskase.com/articles/2019/10/19/how-to-test-api-usability-part-2/><title>How to test API usability: part 2 | aviskase</title>
<meta name=viewport content="width=device-width,initial-scale=1"><meta charset=utf-8><meta property="og:title" content="How to test API usability: part 2"><meta name=twitter:title content="How to test API usability: part 2"><meta property="og:description" content="This is part two of a two-parter. Check out part one.

  Empirical methods

The distinction between analytical and empirical methods is that the latter investigates how real users will use the product.
But don&rsquo;t assume that empirical methods are by default better than analytical: both are important because they discover different problems. This research showed that heuristics were more efficient in finding documentation and structural problems, whereas empirical methods were more useful in finding UX and runtime specific issues."><meta property="twitter:description" content="This is part two of a two-parter. Check out part one.

  Empirical methods

The distinction between analytical and empirical methods is that the latter investigates how real users will use the product.
But don&rsquo;t assume that empirical methods are by default better than analytical: both are important because they discover different problems. This research showed that heuristics were more efficient in finding documentation and structural problems, whereas empirical methods were more useful in finding UX and runtime specific issues."><meta name=description content="This is part two of a two-parter. Check out part one.

  Empirical methods

The distinction between analytical and empirical methods is that the latter investigates how real users will use the product.
But don&rsquo;t assume that empirical methods are by default better than analytical: both are important because they discover different problems. This research showed that heuristics were more efficient in finding documentation and structural problems, whereas empirical methods were more useful in finding UX and runtime specific issues."><meta property="og:url" content="https://www.aviskase.com/articles/2019/10/19/how-to-test-api-usability-part-2/"><link rel=canonical href=https://www.aviskase.com/articles/2019/10/19/how-to-test-api-usability-part-2/><meta property="og:type" content="article"><meta property="article:published_time" content="2019-10-19T00:00:00+00:00"><meta property="article:modified_time" content="2019-10-19T00:00:00+00:00"><meta property="og:site_name" content="aviskase"><meta property="article:author" content="https://www.aviskase.com/pages/about/"><meta name=twitter:card content="summary"><meta name=twitter:creator content="@aviskase"><link rel=me href=https://infosec.exchange/@aviskase><meta name=fediverse:creator content="@aviskase@infosec.exchange"><meta name=generator content="Hugo 0.147.3"><link rel=stylesheet href=/css/main.min.b63568c3305c9881a652d576d84e653067af3f52f79a9a4303ea2c5ea74cf65d.css integrity="sha256-tjVowzBcmIGmUtV22E5lMGevP1L3mppDA+osXqdM9l0="><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest></head><body><header><a href=/ class=title>aviskase</a><p><a href=/>Home</a>
<a href=/pages/about/>About</a>
<a href=/pages/archive/>Archive</a>
<a href=/feeds/all.atom.xml>RSS</a></p></header><main><div class=title><h1>How to test API usability: part 2</h1></div><table class=article-props><tr><th class=width-min>published</th><td class=width-auto>2019-10-19</td></tr><tr><th class=width-min>reading&nbsp;time</th><td class=width-auto>6&nbsp;mins</td></tr><tr><th class=width-min>categories</th><td class=width-auto><a href=/categories/it/>it</a></td></tr></table><article><p>This is part two of a two-parter. Check out <a href=https://www.aviskase.com/articles/2019/10/13/how-to-test-api-usability-part-1/ class=internal-link>part one</a>.</p><h2 id=empirical-methods><a href=#empirical-methods>Empirical methods</a></h2><p>The distinction between analytical and empirical methods is that the latter investigates how real users will use the product.</p><p>But don&rsquo;t assume that empirical methods are by default better than analytical: both are important because they discover different problems. <a href=https://link.springer.com/content/pdf/10.1007%2F978-3-642-34347-6_10.pdf class=external-link>This research</a> showed that heuristics were more efficient in finding documentation and structural problems, whereas empirical methods were more useful in finding UX and runtime specific issues.</p><p><figure aria-label="Barchart with comparison of different issue types found via different methods"><img alt="Barchart with comparison of different issue types found via different methods" title="Barchart with comparison of different issue types found via different methods" width=441 height=232 srcset="/articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_barchart_hu_b903ddfd714e1c29.webp 441w, /articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_barchart_hu_b33503e8c8b20ff3.webp 420w," sizes="(min-width: 800px) 50vw, 100vw" src=/articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_barchart_hu_29db031b13e4380e.jpg decoding=async loading=lazy></figure></p><h3 id=monitoring><a href=#monitoring>Monitoring</a></h3><p>Monitoring is used to gather usage statistics. For web services, it&rsquo;s rather easy. For instance, you can discover that one API endpoint is never called. Hence, you should consider the causes: is it missing in the documentation or not needed to anyone? Monitoring also helps to map scenarios: what kind of requests, to which services, and in what order happen most often.</p><p>And don&rsquo;t forget to monitor not only successful requests but also failures. Imagine, some business errors are stockpiling: maybe you need to reconsider API design or error handling?</p><p>Another thing to monitor is data volumes. Analysts from the project I worked on assumed that type A documents should be more common than type B documents, so the service was better optimized for the first type. It was quite a surprise when we did a simple SQL count and found out that the number of type A documents were 600 thousand, while type B accounted for 80 million. After that discovery, we had to prioritize tasks related to service B way higher.</p><h3 id=support-tickets><a href=#support-tickets>Support tickets</a></h3><p>If you have a support team, you&rsquo;re in luck: analyze tickets, pick out those related to usability, and identify the most serious issues. Previously I <a href=https://www.aviskase.com/articles/2019/09/02/your-api-is-your-public-image/ class=internal-link>wrote about accidental Cyrillic symbols instead of English in service schema</a>: these problems resurfaced specifically via support.</p><p>Moreover, support tickets offer insight into the most common tools and workflows used to work with your API. Once we had an external developer who generated <code>soapAction</code> dynamically based on a root request structure by trimming the word <code>Request</code>. For example, <code>importHouseRequest</code> gave <code>importHouse</code>. But one of our services with a name <code>importPaymentDocumentRequest</code> expected <code>soapAction=importPaymentDocumentData</code> instead of <code>importPaymentDocument</code> (what the developer would expect). On the one hand, the user&rsquo;s solution was poor: you&rsquo;d better use WSDL. On the other hand, maybe they didn’t have a choice and we probably should ask ourselves why naming wasn&rsquo;t consistent.</p><h3 id=surveys><a href=#surveys>Surveys</a></h3><p>Not everyone has a support service. Or perhaps it doesn&rsquo;t give enough information. In that case, surveying API users is helpful. There is no point in giving examples: this topic is highly contextual. But you can start with the basics: &ldquo;What do you like?&rdquo;, &ldquo;What do you don&rsquo;t like?&rdquo;, and &ldquo;What would you like to change?&rdquo;.</p><h3 id=user-sessions><a href=#user-sessions>User sessions</a></h3><p>User sessions are the most expensive and cumbersome usability evaluation method. You need to find people based on a typical user profile, give them some tasks, watch the process, and analyze results.</p><p>Each company administers sessions in its own way. <a href=http://apiux.com/2014/06/10/usability-dropbox-api/ class=external-link>Some perform remote sessions</a>, <a href=http://blog.pamelafox.org/2012/03/api-usability-testing.html class=external-link>others invite developers on site</a>. In both cases developers can use their own laptops and favorite IDEs: first, it&rsquo;s closer to real-world conditions, second, it minimizes stress from an unknown environment.</p><p>Yet, there are more exotic examples. A developer is led to the room with a one-way mirror (yup, like in movies). A usability expert sits behind the mirror and observes developer actions as well as what&rsquo;s happening on the dev&rsquo;s computer screen. The developer can ask questions, but the expert will answer only in rare cases. In my opinion, it&rsquo;s too sterile.</p><p>Generally, all API related user sessions have two phases. The first phase is <strong>a developer workshop</strong> with tasks like:</p><ul><li>Solve a problem in the notebook without an IDE (to get an idea of how developers would design API on their own).</li><li>Practical tasks for API usage (e.g., write a code for file upload using a service).</li><li>Read and review a code snippet to assess its clarity and readability (use printouts to make this task more challenging).</li><li>Debug a faulty code snippet (this helps to study how a user will handle and correct an error).</li></ul><p>The second phase is <strong>an interview</strong> where you ask:</p><ul><li>Name three biggest issues you encountered during the workshop and how did you overcome them (documentation, support, StackOverflow, a friend&rsquo;s help)?</li><li>How much time did you spend looking for additional information outside official documentation?</li><li>Did you encounter unexpected error messages? If yes, did they help you correct a problem?</li><li>Name at least three ways to improve official documentation.</li><li>Name at least three ways to improve API design.</li></ul><h2 id=personas><a href=#personas>Personas</a></h2><p>Personas are used both in analytical and empirical methods. All you need is to figure out which characteristics best describe your users (in case of API, developers). These descriptions tend to be humanized by assigning a name and a photo, adding information about fears and preferences. You can wear a &ldquo;persona hat&rdquo; while applying heuristics or rely on them while selecting developers for user sessions.</p><p>Typical developers&rsquo; personas:</p><ul><li><strong>Systematic developers</strong> don&rsquo;t trust API and write code defensively. They are usually deductive, write on C++, C, or even Assembly.</li><li><strong>Pragmatic developers</strong> are more common and work both in deductive and inductive manners. Typically they code desktop and mobile apps in Java or C#.</li><li><strong>Opportunistic developers</strong> concentrate on solving business problems in an exploratory and inductive fashion. Guess what language they prefer? JavaScript.</li></ul><div class="callout warning"><p>Now, I want to point out that the aforementioned language discrimination is not my invention. If you&rsquo;re lucky, perhaps you&rsquo;ll find the original article by Visual Studio usability expert, where these quirky definitions were introduced. Unfortunately, I was able to get only [its first page in the Wayback Machine][visual-studio], so you have to take my word for it. Nevertheless, I hope this example can encourage you to create your own personas.</p></div><p>We can also combine personas with cognitive dimensions. Create <a href=https://en.wikipedia.org/wiki/Radar_chart class=external-link>a radar chart</a> with 12 axes, where each axis is a cognitive dimension. Next, plot current values for your API and values according to the persona&rsquo;s expectations. This chart is great for comparing how existing API corresponds to user values.</p><p><figure aria-label="Radar chart with comparison of developer expectations vs current state of API"><img alt="Radar chart with comparison of developer expectations vs current state of API" title="Radar chart with comparison of developer expectations vs current state of API" width=286 height=286 srcset="/articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_chart_hu_5bb3f89b2257838d.webp 286w," sizes="(min-width: 800px) 50vw, 100vw" src=/articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_chart_hu_7fc42d56485b97a.jpg decoding=async loading=lazy></figure></p><p>Developer from the example chart (blue line) prefers API with a high level of consistency (10) and hates writing boilerplate (4). As we can see, the current state of API (black line) doesn&rsquo;t satisfy these criteria.</p><h2 id=summing-up><a href=#summing-up>Summing up</a></h2><p>Readers comfortable with GUI usability testing would say: &ldquo;That&rsquo;s exactly the same stuff!&rdquo;. And you&rsquo;re right, there is nothing supernatural about API usability. Even though it&rsquo;s called an <em>application programming</em> interface, programs are yet to learn how to find other APIs and use them automatically; they still need us, meatbags. That&rsquo;s why almost everything applied for GUI usability evaluation is reusable for API with some adjustments.</p><p>Now, what about <em>the best method?</em> None, apply them all! <a href=https://link.springer.com/content/pdf/10.1007%2F978-3-642-34347-6_10.pdf class=external-link>According to this research</a>, each method can identify unique issues.</p><p><figure aria-label="Venn diagram showing how different methods overlap in finding different issues"><img alt="Venn diagram showing how different methods overlap in finding different issues" title="Venn diagram showing how different methods overlap in finding different issues" width=457 height=207 srcset="/articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_stats_hu_5108567ee1b4f7cb.webp 457w, /articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_stats_hu_d25cf3a352b3cd83.webp 420w," sizes="(min-width: 800px) 50vw, 100vw" src=/articles/2019/10/19/how-to-test-api-usability-part-2/api_ux_stats_hu_9991ac89369c1322.jpg decoding=async loading=lazy></figure></p><p>If you are tight on resources, I suggest using the least expensive methods: heuristics, cognitive dimensions, walkthrough, and support tickets. Even the simplest techniques can drive API improvements.</p><p>Someone would argue that API usability is not that important: &ldquo;we don&rsquo;t have time for that, it&rsquo;s a dev thingy.&rdquo; But developers created <a href=https://github.com/google/styleguide class=external-link>style guides</a> not just to be fancy; this serves to accelerate the achievement of shippable quality. We care about hidden code quality, therefore we need to care about externally visible code like APIs even more.</p></article><nav class=article-nav aria-label="Post navigation"><span class=article-nav-old><a href=https://www.aviskase.com/articles/2019/10/13/how-to-test-api-usability-part-1/ title="How to test API usability: part 1" aria-label="Go to older post"><span class=arrow>◁</span>older</a>
</span><span class=article-nav-middle>&nbsp;&#183;&nbsp;&#183;&nbsp;&#183;&nbsp;</span>
<span class=article-nav-new><a href=https://www.aviskase.com/articles/2019/10/26/a-testers-guide-on-hunting-for-api-related-sources/ title="A tester’s guide on hunting for API related sources" aria-label="Go to newer post">newer<span class=arrow>▶</span></a></span></nav></main><footer><span>2025</span><a href=https://infosec.exchange/@aviskase rel=me aria-label="Visit my Mastodon profile"><svg role="img" height="24" viewBox="0 0 24 24" width="24"><title>Mastodon</title><desc>An icon representing the Mastodon social network</desc><path d="m20.94 14c-.28 1.41-2.44 2.96-4.97 3.26-1.31.15-2.6.3-3.97.24-2.25-.11-4-.54-4-.54v.62c.32 2.22 2.22 2.35 4.03 2.42 1.82.05 3.44-.46 3.44-.46l.08 1.65s-1.28.68-3.55.81c-1.25.07-2.81-.03-4.62-.5-3.92-1.05-4.6-5.24-4.7-9.5l-.01-3.43c0-4.34 2.83-5.61 2.83-5.61C6.95 2.3 9.41 2 11.97 2h.06c2.56.0 5.02.3 6.47.96.0.0 2.83 1.27 2.83 5.61.0.0.04 3.21-.39 5.43M18 8.91c0-1.08-.3-1.91-.85-2.56-.56-.63-1.3-.96-2.23-.96-1.06.0-1.87.41-2.42 1.23l-.5.88-.5-.88c-.56-.82-1.36-1.23-2.43-1.23-.92.0-1.66.33-2.23.96C6.29 7 6 7.83 6 8.91v5.26h2.1V9.06c0-1.06.45-1.62 1.36-1.62 1 0 1.5.65 1.5 1.93v2.79h2.07V9.37c0-1.28.5-1.93 1.51-1.93.9.0 1.35.56 1.35 1.62v5.11H18z"/></svg></a>
<a href=mailto:aviskase@gmail.com>Yuliya Bagriy</a><a href=https://www.aviskase.com/pages/i-dont-track-you/>No tracking</a></footer></body></html>